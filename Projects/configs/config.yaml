model:
  generator_lr: 0.0002
  discriminator_lr: 0.0002
  betas: [0.5, 0.999]
  lambda_l1: 100.0

data:
  dataset_path: "dataset\\"
  batch_size: 16
  image_size: 256
  num_workers: 4

training:
  # epochs: 100
  log_interval: 10

train:
  lr: 0.0001
  num_epochs: 100
  batch_size: 4
  # lr: 0.0002
  epochs: 50
  dataset_path: "dataset\\"



# data:
#   dataset_path: "./dataset"       # Path to your dataset folder
#   image_size: 256                 # Resize all images to this size
#   batch_size: 16                  # Number of images per training batch
#   num_workers: 4                  # Number of parallel workers for data loading

# training:
#   epochs: 100                     # Total training epochs
#   learning_rate: 0.0002           # Learning rate for optimizers
#   beta1: 0.5                      # Adam optimizer beta1
#   lambda_l1: 100                  # L1 loss weight
#   log_interval: 10                # Log every 10 iterations
#   checkpoint_interval: 10         # Save checkpoints every 10 epochs

# model:
#   lambda_adv: 1.0                 # Weight for adversarial loss
#   lambda_perceptual: 10           # Weight for perceptual loss
